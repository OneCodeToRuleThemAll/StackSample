{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T14:52:40.067057Z","iopub.status.busy":"2024-03-03T14:52:40.066687Z","iopub.status.idle":"2024-03-03T14:52:41.239466Z","shell.execute_reply":"2024-03-03T14:52:41.238384Z","shell.execute_reply.started":"2024-03-03T14:52:40.067027Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import hamming_loss\n","from sklearn.metrics import jaccard_score\n","from sklearn.metrics import precision_recall_fscore_support as score\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn import metrics\n","from sklearn.svm import LinearSVC\n","import transformers\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertModel, BertConfig\n","import seaborn as sns\n","import shutil, sys\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import warnings\n","import torch\n","import os\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import get_scheduler\n","from tqdm.auto import tqdm\n","from torch.nn import BCEWithLogitsLoss\n","from sklearn.metrics import f1_score, jaccard_score\n","\n","\n","# !pip install seaborn\n","\n","train_df = pd.read_pickle('/kaggle/input/stacksample/train_df.pkl')\n","val_df = pd.read_pickle('/kaggle/input/stacksample/val_df.pkl')\n","test_df = pd.read_pickle('/kaggle/input/stacksample/test_df.pkl')\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using {device} as device \\n')\n","\n","tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n","model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=91)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-03T14:52:41.243867Z","iopub.status.busy":"2024-03-03T14:52:41.243167Z"},"trusted":true},"outputs":[],"source":["# Define a max length for padding/truncation\n","MAX_LENGTH = 512\n","batch_size = 36\n","\n","# Tokenization\n","def tokenize_function(examples):\n","    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n","\n","# Tokenize the text data in the DataFrames\n","train_encodings = tokenize_function(train_df['input'].tolist())\n","val_encodings = tokenize_function(val_df['input'].tolist())\n","test_encodings = tokenize_function(test_df['input'].tolist())\n","\n","# Define custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx] for key, val in self.encodings.items()}\n","        item['Tag'] = torch.tensor(self.labels[idx], dtype=torch.float) # FLOAT NEEDED FOR BCELOSS\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Create datasets\n","train_dataset = CustomDataset(train_encodings, train_df['Tag'].values)\n","val_dataset = CustomDataset(val_encodings, val_df['Tag'].values)\n","test_dataset = CustomDataset(test_encodings, test_df['Tag'].values)\n","\n","# Create dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Define the optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","\n","# Epochs / Learning Rate Scheduler\n","num_epochs = 15\n","num_epochs=2\n","best_valid_loss = float('inf')\n","patience, trials = 2, 0  # patience: number of epochs to wait for improvement before stopping, trials: count of epochs without improvement\n","\n","num_training_steps = num_epochs * len(train_loader)\n","lr_scheduler = get_scheduler(\n","    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",")\n","\n","# Assuming your model is correctly set up for multi-label classification:\n","model.to(device)  # Make sure to send your model to the right device\n","\n","# Define the loss function for multi-label classification\n","loss_fn = BCEWithLogitsLoss()\n","progress_bar = tqdm(range(num_training_steps))\n","\n","for epoch in range(num_epochs):\n","    train_loss = 0.0\n","    model.train()  # Ensure the model is in training mode\n","    \n","    for batch in train_loader:\n","        # Separate inputs from labels and send them to the device\n","        inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n","        labels = batch['Tag'].to(device)\n","        \n","        # Forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        \n","        # Calculate loss\n","        loss = loss_fn(logits, labels)\n","        \n","        # Backward pass and optimize\n","        optimizer.zero_grad() # just not between the backward and step\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        \n","        train_loss += loss.item()\n","        \n","        progress_bar.update(1)\n","        \n","    train_loss /= len(train_loader)\n","    \n","    # Validation phase\n","    model.eval() # Disaple dropout, batch normalization to moving average instead of batch average\n","    valid_loss = 0\n","    with torch.no_grad(): # Disable grads req and graph \n","        for batch in val_loader:\n","            inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n","            labels = batch['Tag'].to(device)\n","            outputs = model(**inputs)\n","            loss = loss_fn(outputs.logits, labels.float())\n","            valid_loss += loss.item()\n","    \n","    valid_loss /= len(val_loader)\n","    \n","    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')\n","    \n","    # Early stopping logic\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'best_model.pt')  # Save your best model\n","        trials = 0\n","    else:\n","        trials += 1\n","        if trials >= patience:\n","            print(f'Early stopping on epoch {epoch+1}.')\n","            break  # Stop training\n","\n","    # Optionally, here you can save your checkpoints\n","    # save_ckp({'epoch': epoch+1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, False, checkpoint_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set the model to evaluation mode\n","model.eval()\n","\n","# Store predictions and true labels\n","predictions = []\n","true_labels = []\n","\n","# Disable gradient calculation for evaluation to save memory and computations\n","with torch.no_grad():\n","    for batch in test_loader:\n","        inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n","        labels = batch['Tag'].to(device)\n","\n","        # Forward pass, get logits\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","\n","        # Apply sigmoid to logits to get predictions in [0,1]\n","        probs = torch.sigmoid(logits).cpu().numpy()\n","        # Convert probabilities to binary predictions\n","        batch_preds = np.where(probs > 0.5, 1, 0)\n","\n","        predictions.append(batch_preds)\n","        true_labels.append(labels.cpu().numpy())\n","\n","# Concatenate all the batches\n","predictions = np.vstack(predictions)\n","true_labels = np.vstack(true_labels)\n","\n","# Calculate Jaccard Score\n","jaccard = jaccard_score(true_labels, predictions, average='samples')  # 'samples' for multilabel classification\n","\n","# Calculate micro-averaged F1 score\n","micro_f1 = f1_score(true_labels, predictions, average='micro')\n","\n","print(f'Jaccard Score: {jaccard}')\n","print(f'Micro-averaged F1 Score: {micro_f1}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Function to evaluate the model\n","def evaluate_model(model, data_loader, device):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n","            labels = batch['Tag'].to(device)\n","            outputs = model(**inputs)\n","            logits = outputs.logits\n","            pred = torch.sigmoid(logits) > 0.5 # Using sigmoid since it's BCEWithLogitsLoss\n","            predictions.append(pred.cpu().numpy())\n","            actuals.append(labels.cpu().numpy())\n","\n","    # Concatenate all batches\n","    all_predictions = np.vstack(predictions)\n","    all_actuals = np.vstack(actuals)\n","\n","    # Calculate Jaccard Score and Micro F1 Score\n","    jaccard = jaccard_score(all_actuals, all_predictions, average='samples')\n","    micro_f1 = f1_score(all_actuals, all_predictions, average='micro')\n","    \n","    return jaccard, micro_f1\n","\n","# Load the best model\n","model.load_state_dict(torch.load('best_model.pt'))\n","\n","# Evaluate the model\n","jaccard, micro_f1 = evaluate_model(model, test_loader, device)\n","\n","print(f\"Jaccard Score on Test Set: {jaccard:.1f}\")\n","print(f\"Micro F1 Score on Test Set: {micro_f1:.1f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4531326,"sourceId":7750518,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
