{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7750518,"sourceType":"datasetVersion","datasetId":4531326}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.metrics import hamming_loss\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn import metrics\nfrom sklearn.svm import LinearSVC\nimport transformers\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertModel, BertConfig\nimport seaborn as sns\nimport shutil, sys\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport torch\nimport os\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import get_scheduler\nfrom tqdm.auto import tqdm\nfrom torch.nn import BCEWithLogitsLoss\nfrom sklearn.metrics import f1_score, jaccard_score\n\n\ntrain_df = pd.read_pickle('/kaggle/input/stacksample/train_df.pkl')\nval_df = pd.read_pickle('/kaggle/input/stacksample/val_df.pkl')\ntest_df = pd.read_pickle('/kaggle/input/stacksample/test_df.pkl')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using {device} as device \\n')\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\nmodel = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=91)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:53:02.333665Z","iopub.execute_input":"2024-03-03T21:53:02.334386Z","iopub.status.idle":"2024-03-03T21:53:15.451769Z","shell.execute_reply.started":"2024-03-03T21:53:02.334355Z","shell.execute_reply":"2024-03-03T21:53:15.450886Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Using cuda as device \n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"569e4b64c78c4d9c9c9a7f0457d2b0b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c28a5df7186410985d9f88a557b8f54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a730c17cc41e4adcb16da2d64d1ddba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b5868267c294ebeb166e28abce89e11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d80a00be7f4d47acc9a46807748a01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40266f77c8ce43ceb7a074696ff5ded3"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define a max length for padding/truncation\nMAX_LENGTH = 512\nbatch_size = 36\n\n# Tokenization\ndef tokenize_function(examples):\n    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n\n# Tokenize the text data in the DataFrames\ntrain_encodings = tokenize_function(train_df['input'].tolist())\nval_encodings = tokenize_function(val_df['input'].tolist())\ntest_encodings = tokenize_function(test_df['input'].tolist())\n\n# Define custom dataset\nclass CustomDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['Tag'] = torch.tensor(self.labels[idx], dtype=torch.float) # FLOAT NEEDED FOR BCELOSS\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create datasets\ntrain_dataset = CustomDataset(train_encodings, train_df['Tag'].values)\nval_dataset = CustomDataset(val_encodings, val_df['Tag'].values)\ntest_dataset = CustomDataset(test_encodings, test_df['Tag'].values)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define the optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Epochs / Learning Rate Scheduler\nnum_epochs = 15\nnum_epochs=2\nbest_valid_loss = float('inf')\npatience, trials = 2, 0  # patience: number of epochs to wait for improvement before stopping, trials: count of epochs without improvement\n\nnum_training_steps = num_epochs * len(train_loader)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)\n\n# Assuming your model is correctly set up for multi-label classification:\nmodel.to(device)  # Make sure to send your model to the right device\n\n# Define the loss function for multi-label classification\nloss_fn = BCEWithLogitsLoss()\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_epochs):\n    train_loss = 0.0\n    model.train()  # Ensure the model is in training mode\n    \n    for batch in train_loader:\n        # Separate inputs from labels and send them to the device\n        inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n        labels = batch['Tag'].to(device)\n        \n        # Forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n        \n        # Calculate loss\n        loss = loss_fn(logits, labels)\n        \n        # Backward pass and optimize\n        optimizer.zero_grad() # just not between the backward and step\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        \n        train_loss += loss.item()\n        \n        progress_bar.update(1)\n        \n    train_loss /= len(train_loader)\n    \n    # Validation phase\n    model.eval() # Disaple dropout, batch normalization to moving average instead of batch average\n    valid_loss = 0\n    with torch.no_grad(): # Disable grads req and graph \n        for batch in val_loader:\n            inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n            labels = batch['Tag'].to(device)\n            outputs = model(**inputs)\n            loss = loss_fn(outputs.logits, labels.float())\n            valid_loss += loss.item()\n    \n    valid_loss /= len(val_loader)\n    \n    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')\n    \n    # Early stopping logic\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'best_model.pt')  # Save your best model\n        trials = 0\n    else:\n        trials += 1\n        if trials >= patience:\n            print(f'Early stopping on epoch {epoch+1}.')\n            break  # Stop training\n\n    # Optionally, here you can save your checkpoints\n    # save_ckp({'epoch': epoch+1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, False, checkpoint_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:53:15.453864Z","iopub.execute_input":"2024-03-03T21:53:15.454181Z","iopub.status.idle":"2024-03-03T21:56:16.930930Z","shell.execute_reply.started":"2024-03-03T21:53:15.454155Z","shell.execute_reply":"2024-03-03T21:56:16.929800Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/176 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06686f037c44534b483f991ccde14e7"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, Train Loss: 0.5564, Valid Loss: 0.4586\nEpoch 2, Train Loss: 0.4311, Valid Loss: 0.4075\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Store predictions and true labels\npredictions = []\ntrue_labels = []\n\n# Disable gradient calculation for evaluation to save memory and computations\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n        labels = batch['Tag'].to(device)\n\n        # Forward pass, get logits\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n        # Apply sigmoid to logits to get predictions in [0,1]\n        probs = torch.sigmoid(logits).cpu().numpy()\n        # Convert probabilities to binary predictions\n        batch_preds = np.where(probs > 0.5, 1, 0)\n\n        predictions.append(batch_preds)\n        true_labels.append(labels.cpu().numpy())\n\n# Concatenate all the batches\npredictions = np.vstack(predictions)\ntrue_labels = np.vstack(true_labels)\n\n# Calculate Jaccard Score\njaccard = jaccard_score(true_labels, predictions, average='samples')  # 'samples' for multilabel classification\n\n# Calculate micro-averaged F1 score\nmicro_f1 = f1_score(true_labels, predictions, average='micro')\n\nprint(f'Jaccard Score: {jaccard}')\nprint(f'Micro-averaged F1 Score: {micro_f1}')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T21:56:16.932148Z","iopub.execute_input":"2024-03-03T21:56:16.932429Z","iopub.status.idle":"2024-03-03T21:56:25.939803Z","shell.execute_reply.started":"2024-03-03T21:56:16.932405Z","shell.execute_reply":"2024-03-03T21:56:25.938757Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Jaccard Score: 0.0\nMicro-averaged F1 Score: 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to evaluate the model\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        for batch in data_loader:\n            inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n            labels = batch['Tag'].to(device)\n            outputs = model(**inputs)\n            logits = outputs.logits\n            pred = torch.sigmoid(logits) > 0.5 # Using sigmoid since it's BCEWithLogitsLoss\n            predictions.append(pred.cpu().numpy())\n            actuals.append(labels.cpu().numpy())\n\n    # Concatenate all batches\n    all_predictions = np.vstack(predictions)\n    all_actuals = np.vstack(actuals)\n\n    # Calculate Jaccard Score and Micro F1 Score\n    jaccard = jaccard_score(all_actuals, all_predictions, average='samples')\n    micro_f1 = f1_score(all_actuals, all_predictions, average='micro')\n    \n    return jaccard, micro_f1\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pt'))\n\n# Evaluate the model\njaccard, micro_f1 = evaluate_model(model, test_loader, device)\n\nprint(f\"Jaccard Score on Test Set: {jaccard:.1f}\")\nprint(f\"Micro F1 Score on Test Set: {micro_f1:.1f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}