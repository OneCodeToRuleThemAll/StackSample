{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7757259,"sourceType":"datasetVersion","datasetId":4531326}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.metrics import hamming_loss\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn import metrics\nimport transformers\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import BertTokenizer, BertModel, BertConfig\nimport seaborn as sns\nimport shutil, sys\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport torch\nimport os\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import get_scheduler\nfrom tqdm.auto import tqdm\nfrom torch.nn import BCEWithLogitsLoss\nfrom sklearn.metrics import f1_score, jaccard_score\n\n\ntrain_df = pd.read_pickle('/kaggle/input/stacksample/train_df.pkl')\nval_df = pd.read_pickle('/kaggle/input/stacksample/val_df.pkl')\ntest_df = pd.read_pickle('/kaggle/input/stacksample/test_df.pkl')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using {device} as device \\n')\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\nmodel = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=100)\n# print(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T09:48:28.814681Z","iopub.execute_input":"2024-03-04T09:48:28.815063Z","iopub.status.idle":"2024-03-04T09:48:30.779707Z","shell.execute_reply.started":"2024-03-04T09:48:28.815033Z","shell.execute_reply":"2024-03-04T09:48:30.778820Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Using cuda as device \n\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_model(model, train_dataset, val_dataset, num_epochs=5, batch_size=36, learning_rate=1e-5, patience=2, device='cuda'):\n    # Create DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    # Move model to the correct device\n    model.to(device)\n\n    # Define the optimizer and loss function\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n    loss_fn = BCEWithLogitsLoss()\n\n    # Initialize variables for early stopping\n    best_valid_loss = float('inf')\n    trials = 0\n\n    num_training_steps = num_epochs * len(train_loader)\n    progress_bar = tqdm(total=num_training_steps, desc=\"Training\")\n\n    for epoch in range(num_epochs):\n        model.train()  # Set model to training mode\n        train_loss = 0\n\n        for batch in train_loader:\n            optimizer.zero_grad()\n            inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n            labels = batch['Tag'].to(device)\n\n            outputs = model(**inputs)\n            loss = loss_fn(outputs.logits, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            progress_bar.update(1)\n\n        train_loss /= len(train_loader)\n\n        # Validation phase\n        model.eval()  # Set model to evaluation mode\n        valid_loss = 0\n\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n                labels = batch['Tag'].to(device)\n\n                outputs = model(**inputs)\n                loss = loss_fn(outputs.logits, labels)\n                valid_loss += loss.item()\n\n        valid_loss /= len(val_loader)\n        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')\n\n        # Check for early stopping\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'best_model.pt')\n            trials = 0\n        else:\n            trials += 1\n            if trials >= patience:\n                print(f'Early stopping triggered at epoch {epoch+1}.')\n                break\n\n    progress_bar.close()\n\n# Example usage\ntrain_model(model, train_dataset, val_dataset, num_epochs=25, batch_size=36, learning_rate=2e-5, patience=2, device=device)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:23:37.984542Z","iopub.execute_input":"2024-03-04T10:23:37.985554Z","iopub.status.idle":"2024-03-04T10:27:52.942255Z","shell.execute_reply.started":"2024-03-04T10:23:37.985521Z","shell.execute_reply":"2024-03-04T10:27:52.941370Z"},"trusted":true},"execution_count":37,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/3475 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4fa181d723b40a7918ac63447623983"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, Train Loss: 0.0157, Valid Loss: 0.0333\nEpoch 2, Train Loss: 0.0140, Valid Loss: 0.0329\nEpoch 3, Train Loss: 0.0126, Valid Loss: 0.0329\nEpoch 4, Train Loss: 0.0113, Valid Loss: 0.0331\nEarly stopping triggered at epoch 4.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to evaluate the model\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        for batch in data_loader:\n            inputs = {k: v.to(device) for k, v in batch.items() if k != 'Tag'}\n            labels = batch['Tag'].to(device)\n            outputs = model(**inputs)\n            logits = outputs.logits\n            pred = torch.sigmoid(logits) > 0.5 # Using sigmoid since it's BCEWithLogitsLoss\n            predictions.append(pred.cpu().numpy())\n            actuals.append(labels.cpu().numpy())\n\n    # Concatenate all batches\n    all_predictions = np.vstack(predictions)\n    all_actuals = np.vstack(actuals)\n\n    # Calculate Jaccard Score and Micro F1 Score\n    jaccard = jaccard_score(all_actuals, all_predictions, average='samples')\n    micro_f1 = f1_score(all_actuals, all_predictions, average='micro')\n    \n    return jaccard, micro_f1\n\n# Load the best model\nmodel.load_state_dict(torch.load('best_model.pt'))\n\n# Evaluate the model\njaccard, micro_f1 = evaluate_model(model, test_loader, device)\n\nprint(f\"Jaccard Score on Test Set: {jaccard:.2f}\")\nprint(f\"Micro F1 Score on Test Set: {micro_f1:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T10:30:24.586186Z","iopub.execute_input":"2024-03-04T10:30:24.586552Z","iopub.status.idle":"2024-03-04T10:30:28.697677Z","shell.execute_reply.started":"2024-03-04T10:30:24.586524Z","shell.execute_reply":"2024-03-04T10:30:28.696723Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Jaccard Score on Test Set: 0.58\nMicro F1 Score on Test Set: 0.65\n","output_type":"stream"}]},{"cell_type":"markdown","source":"XGBOOST performance on the Test Dataset:\n\n    Micro F1 Score: 0.31\n\n    Jaccard Score: 0.21\n\nDistilBERT performance on the Test Dataset:\n\n    Micro F1 Score: 0.65\n\n    Jaccard Score: 0.58\n    \n    \nPerformance Overview:\n        XGBoost scored lower with a Micro F1 Score of 0.31 and a Jaccard Score of 0.21, indicating challenges in precision, recall, and label similarity. DistilBERT significantly outperformed XGBoost, achieving a Micro F1 Score of 0.65 and a Jaccard Score of 0.58, showcasing superior accuracy and label matching.\n\nModel Insights:\n        XGBoost is optimal for tabular data but struggled in this context, possibly due to complex patterns or text data. DistilBERT excels in natural language processing, capturing semantic relationships effectively, which explains its higher performance and at the same time is a distilled/smaller version of the BERT model. (Meaning if we wanted, we could push for more performance)\n\nChoosing the Right Model:\n        The choice between XGBoost and DistilBERT should be based on data type (structured vs. text) and available computational resources, with DistilBERT requiring more. If we are aiming for performance, the go to choice should be the transformers model.\n","metadata":{}}]}